{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTT4185 Machine learning for Speech technology\n",
    "\n",
    "## Computer assigment 3b:  Regression analysis\n",
    "\n",
    "Regression analysis is used to estimate/measure the relationship between an _independent_ variable, say $x$, and a _dependent_ variable, say $y$. One of the simplest regression problems is \n",
    "\\begin{equation}\n",
    "y = ax + b\n",
    "\\end{equation}\n",
    "where $a$ and $b$ are constants. In practice our observations will be contaminated by noise, so we have\n",
    "\\begin{equation}\n",
    "y = ax + b + n,\n",
    "\\end{equation}\n",
    "where $n$ is noise, eg. measurement errors. This particular problem is called _linear regression_.\n",
    "\n",
    "We will have a look at _non-linear regression_, using deep neural networks. Here we are looking at general regression problems in the form \n",
    "\\begin{equation}\n",
    "y = f(x) + n.\n",
    "\\end{equation}\n",
    "\n",
    "We generate our data according to the function $f(x) = x^2 + \\cos(20x) \\text{ sign}(x)$, obtaining a set of observations $\\{(x_i,y_i)\\}$.\n",
    "\n",
    "Then we assume we do not know the underlying function and we try to recover and approximation of $f$ only using the observations $\\{(x_i,y_i)\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def f(x):\n",
    "    return x**2 + np.cos(20*x)*np.sign(x)\n",
    "\n",
    "# Setup some simulation parameters\n",
    "# Number of observations\n",
    "N = 5000\n",
    "\n",
    "# Plot a \"clean\" version of the relationship between x and y\n",
    "plt.figure(figsize=(10, 8))\n",
    "x = np.linspace(-2,2,N)\n",
    "plt.plot(x,f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noise version of the observations\n",
    "y = f(x) + np.random.randn(len(x))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to perform regression is to assume that the data is generated using a set of functions from a cerain family, for example polynomials of order $p$,\n",
    "\\begin{equation}\n",
    "\\hat f(x) = a_0 + a_1 x + a_2 x^2 \\ldots a_p x^p.\n",
    "\\end{equation}\n",
    "Then regression corresponds to fitting the parameters in the model. Let us see how this works out before using our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a set of polynomial orders to try\n",
    "P = [1, 2, 5, 10, 20]\n",
    "\n",
    "# Define estimator function. Arguments are inout variable, observation and polynomial order\n",
    "# Returns a set of polynomial coefficients\n",
    "def reg_estimator(x,y,p):\n",
    "    # Use simple ls approach\n",
    "    N = len(x)\n",
    "    H = np.zeros((N,p+1))\n",
    "    for col in range(p+1):\n",
    "        H[:,col] = x**col\n",
    "    iHtH = np.linalg.inv(np.dot(H.T,H))\n",
    "    theta = np.dot(np.dot(iHtH,H.T),y)\n",
    "    return theta\n",
    "\n",
    "# Computes fx) = c_0 + c_1x + c_2 x^2 ... c_p x^p\n",
    "def poly(x, C):\n",
    "    # compute p(x) for coeffs in c\n",
    "    y = 0*x\n",
    "    for p, c in enumerate(C):\n",
    "        y += c*x**p        \n",
    "    return y\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,f(x),label=\"Truth\")\n",
    "for p in P:\n",
    "    C = reg_estimator(x,y,p)\n",
    "    plt.plot(x,poly(x,C),label=\"Poly order \" + str(p))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Play with different $p$ to see how close you can get to the true function.\n",
    "\n",
    "Note: Very high $p$ will give numerical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will use a deep neural network to approximate $f$. We set this up below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ceate a model with a single hidden layer. Note that input and output has\n",
    "# dimension one\n",
    "M = 512\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(M, activation=tf.nn.relu, input_dim=1),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# Train the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network by using $x$ as an input and the squared error between the network output $\\hat y$ and the observed value $y$ as a loss\n",
    "\\begin{equation}\n",
    " L = \\frac{1}{N} \\sum_n (\\hat y - y)^2\n",
    "\\end{equation}\n",
    "\n",
    "We first try our network on clean data to check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(x, f(x), epochs=1000, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the variable `history`, plot the evolution of the loss during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute \\hat y from the network and compare this to the true f(x)\n",
    "z = model.predict(x)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,f(x),label=\"Truth\")\n",
    "plt.plot(x,z,label=\"DNN\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Try increasing the number of nodes in the network to see if the results can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use a deep network with more than one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with multiple hidden layers. Note that input and output has\n",
    "# dimension one\n",
    "M = 16\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(M, activation=tf.nn.relu, input_dim=1),\n",
    "    keras.layers.Dense(M, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(M, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(M, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x, f(x), epochs=1000, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.predict(x)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,f(x),label=\"Truth\")\n",
    "plt.plot(x,z,label=\"DNN\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Try increasing the number of hidden nodes per layer until performance is satisfactory. Can the same effect be achieved by just adding more layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Using the best setup from the previous problem, train a model using the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
